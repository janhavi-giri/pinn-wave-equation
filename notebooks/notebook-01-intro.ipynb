{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Physics-Informed Neural Networks (PINNs)\n",
    "\n",
    "This notebook provides a comprehensive introduction to Physics-Informed Neural Networks, a revolutionary approach to solving partial differential equations using deep learning.\n",
    "\n",
    "## Table of Contents\n",
    "1. [What are PINNs?](#what-are-pinns)\n",
    "2. [How do PINNs work?](#how-do-pinns-work)\n",
    "3. [Advantages over traditional methods](#advantages)\n",
    "4. [Simple example: Solving a 1D ODE](#simple-example)\n",
    "5. [Key concepts](#key-concepts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML, display\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What are PINNs? <a id='what-are-pinns'></a>\n",
    "\n",
    "Physics-Informed Neural Networks (PINNs) are a class of neural networks that are trained to solve supervised learning tasks while respecting any given laws of physics described by general nonlinear partial differential equations.\n",
    "\n",
    "### Traditional Approach vs PINNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Traditional vs PINN approach\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Traditional approach\n",
    "ax1.set_title('Traditional Numerical Methods', fontsize=16, weight='bold')\n",
    "ax1.text(0.5, 0.8, 'Discretize Domain', ha='center', fontsize=14, \n",
    "         bbox=dict(boxstyle='round,pad=0.5', facecolor='lightcoral'))\n",
    "ax1.text(0.5, 0.6, '↓', ha='center', fontsize=20)\n",
    "ax1.text(0.5, 0.4, 'Solve Linear System', ha='center', fontsize=14,\n",
    "         bbox=dict(boxstyle='round,pad=0.5', facecolor='lightblue'))\n",
    "ax1.text(0.5, 0.2, '↓', ha='center', fontsize=20)\n",
    "ax1.text(0.5, 0.0, 'Solution at Grid Points', ha='center', fontsize=14,\n",
    "         bbox=dict(boxstyle='round,pad=0.5', facecolor='lightgreen'))\n",
    "ax1.axis('off')\n",
    "\n",
    "# PINN approach\n",
    "ax2.set_title('Physics-Informed Neural Networks', fontsize=16, weight='bold')\n",
    "ax2.text(0.5, 0.8, 'Neural Network', ha='center', fontsize=14,\n",
    "         bbox=dict(boxstyle='round,pad=0.5', facecolor='lightcoral'))\n",
    "ax2.text(0.5, 0.6, '↓', ha='center', fontsize=20)\n",
    "ax2.text(0.5, 0.4, 'Physics Loss Function', ha='center', fontsize=14,\n",
    "         bbox=dict(boxstyle='round,pad=0.5', facecolor='lightblue'))\n",
    "ax2.text(0.5, 0.2, '↓', ha='center', fontsize=20)\n",
    "ax2.text(0.5, 0.0, 'Continuous Solution', ha='center', fontsize=14,\n",
    "         bbox=dict(boxstyle='round,pad=0.5', facecolor='lightgreen'))\n",
    "ax2.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. How do PINNs work? <a id='how-do-pinns-work'></a>\n",
    "\n",
    "The key innovation of PINNs is the physics-informed loss function. Instead of requiring labeled data, PINNs use the PDE itself as the \"label\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple neural network architecture\n",
    "class SimplePINN(nn.Module):\n",
    "    def __init__(self, input_dim=1, hidden_dim=50, output_dim=1, num_layers=3):\n",
    "        super(SimplePINN, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "        layers.append(nn.Tanh())\n",
    "        \n",
    "        for _ in range(num_layers - 2):\n",
    "            layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            layers.append(nn.Tanh())\n",
    "        \n",
    "        layers.append(nn.Linear(hidden_dim, output_dim))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# Visualize the architecture\n",
    "model = SimplePINN()\n",
    "print(\"PINN Architecture:\")\n",
    "print(model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Physics-Informed Loss Function\n",
    "\n",
    "The total loss function for a PINN consists of:\n",
    "\n",
    "$$\\mathcal{L} = \\mathcal{L}_{PDE} + \\mathcal{L}_{BC} + \\mathcal{L}_{IC} + \\mathcal{L}_{data}$$\n",
    "\n",
    "Where:\n",
    "- $\\mathcal{L}_{PDE}$: PDE residual loss\n",
    "- $\\mathcal{L}_{BC}$: Boundary condition loss\n",
    "- $\\mathcal{L}_{IC}$: Initial condition loss\n",
    "- $\\mathcal{L}_{data}$: Data fitting loss (if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate automatic differentiation\n",
    "def demonstrate_autograd():\n",
    "    # Create a simple function: f(x) = x^2\n",
    "    x = torch.tensor([2.0], requires_grad=True)\n",
    "    y = x**2\n",
    "    \n",
    "    # Compute derivative: df/dx = 2x\n",
    "    y.backward()\n",
    "    \n",
    "    print(f\"x = {x.item():.2f}\")\n",
    "    print(f\"f(x) = x² = {y.item():.2f}\")\n",
    "    print(f\"df/dx = 2x = {x.grad.item():.2f}\")\n",
    "    print(f\"\\nExpected: df/dx = 2 × {x.item():.2f} = {2 * x.item():.2f}\")\n",
    "\n",
    "demonstrate_autograd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Advantages over Traditional Methods <a id='advantages'></a>\n",
    "\n",
    "PINNs offer several advantages over traditional numerical methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "import pandas as pd\n",
    "\n",
    "comparison_data = {\n",
    "    'Aspect': ['Domain', 'Solution Type', 'Mesh Required', 'Irregular Geometries', \n",
    "               'High Dimensions', 'Inverse Problems', 'Noisy Data'],\n",
    "    'Traditional Methods': ['Discrete Grid', 'Point Values', 'Yes', 'Challenging', \n",
    "                           'Computationally Expensive', 'Difficult', 'Sensitive'],\n",
    "    'PINNs': ['Continuous', 'Continuous Function', 'No', 'Natural', \n",
    "              'Feasible', 'Natural', 'Robust']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(comparison_data)\n",
    "print(\"Comparison: Traditional Methods vs PINNs\")\n",
    "print(\"=\" * 70)\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Simple Example: Solving a 1D ODE <a id='simple-example'></a>\n",
    "\n",
    "Let's solve a simple ordinary differential equation (ODE) to demonstrate PINNs:\n",
    "\n",
    "$$\\frac{du}{dx} + u = 1, \\quad x \\in [0, 1]$$\n",
    "$$u(0) = 0$$\n",
    "\n",
    "The analytical solution is: $u(x) = 1 - e^{-x}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the PINN for ODE\n",
    "class ODE_PINN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ODE_PINN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(1, 20),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(20, 20),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(20, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Physics-informed loss\n",
    "def ode_loss(model, x):\n",
    "    x.requires_grad = True\n",
    "    u = model(x)\n",
    "    \n",
    "    # Compute du/dx using autograd\n",
    "    du_dx = torch.autograd.grad(u, x, grad_outputs=torch.ones_like(u), \n",
    "                                create_graph=True)[0]\n",
    "    \n",
    "    # ODE residual: du/dx + u - 1 = 0\n",
    "    residual = du_dx + u - 1\n",
    "    \n",
    "    # Boundary condition: u(0) = 0\n",
    "    x_bc = torch.tensor([[0.0]], requires_grad=True)\n",
    "    u_bc = model(x_bc)\n",
    "    \n",
    "    # Total loss\n",
    "    loss = torch.mean(residual**2) + (u_bc**2)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the PINN\n",
    "model = ODE_PINN()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training data (collocation points)\n",
    "x_train = torch.rand(100, 1) * 1.0  # Random points in [0, 1]\n",
    "\n",
    "# Training loop\n",
    "losses = []\n",
    "for epoch in range(2000):\n",
    "    optimizer.zero_grad()\n",
    "    loss = ode_loss(model, x_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    if epoch % 400 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss = {loss.item():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Loss history\n",
    "ax1.semilogy(losses)\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training Loss')\n",
    "ax1.grid(True)\n",
    "\n",
    "# Solution comparison\n",
    "x_test = torch.linspace(0, 1, 100).reshape(-1, 1)\n",
    "with torch.no_grad():\n",
    "    u_pred = model(x_test).numpy()\n",
    "\n",
    "x_np = x_test.numpy()\n",
    "u_true = 1 - np.exp(-x_np)\n",
    "\n",
    "ax2.plot(x_np, u_true, 'k--', linewidth=2, label='Analytical')\n",
    "ax2.plot(x_np, u_pred, 'r-', linewidth=2, label='PINN')\n",
    "ax2.set_xlabel('x')\n",
    "ax2.set_ylabel('u(x)')\n",
    "ax2.set_title('ODE Solution')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compute error\n",
    "error = np.mean(np.abs(u_pred - u_true))\n",
    "print(f\"\\nMean Absolute Error: {error:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Key Concepts <a id='key-concepts'></a>\n",
    "\n",
    "### 5.1 Automatic Differentiation\n",
    "\n",
    "The cornerstone of PINNs is automatic differentiation, which allows us to compute exact derivatives of the neural network output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate higher-order derivatives\n",
    "def demonstrate_higher_order_derivatives():\n",
    "    x = torch.tensor([1.0], requires_grad=True)\n",
    "    \n",
    "    # f(x) = x^3\n",
    "    y = x**3\n",
    "    \n",
    "    # First derivative: f'(x) = 3x^2\n",
    "    dy_dx = torch.autograd.grad(y, x, create_graph=True)[0]\n",
    "    \n",
    "    # Second derivative: f''(x) = 6x\n",
    "    d2y_dx2 = torch.autograd.grad(dy_dx, x, create_graph=True)[0]\n",
    "    \n",
    "    print(\"Function: f(x) = x³\")\n",
    "    print(f\"x = {x.item():.2f}\")\n",
    "    print(f\"f(x) = {y.item():.2f}\")\n",
    "    print(f\"f'(x) = {dy_dx.item():.2f} (expected: 3x² = {3 * x.item()**2:.2f})\")\n",
    "    print(f\"f''(x) = {d2y_dx2.item():.2f} (expected: 6x = {6 * x.item():.2f})\")\n",
    "\n",
    "demonstrate_higher_order_derivatives()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Collocation Points\n",
    "\n",
    "PINNs use collocation points - randomly sampled points in the domain where the PDE residual is minimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize collocation points\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# 1D collocation points\n",
    "x_1d = torch.rand(50) * 2 - 1  # [-1, 1]\n",
    "ax1.scatter(x_1d, torch.zeros_like(x_1d), alpha=0.6, s=50)\n",
    "ax1.set_xlim(-1.1, 1.1)\n",
    "ax1.set_ylim(-0.5, 0.5)\n",
    "ax1.set_xlabel('x')\n",
    "ax1.set_title('1D Collocation Points')\n",
    "ax1.grid(True)\n",
    "\n",
    "# 2D collocation points\n",
    "n_points = 200\n",
    "x_2d = torch.rand(n_points) * 2 - 1\n",
    "y_2d = torch.rand(n_points) * 2 - 1\n",
    "ax2.scatter(x_2d, y_2d, alpha=0.6, s=30)\n",
    "ax2.set_xlim(-1.1, 1.1)\n",
    "ax2.set_ylim(-1.1, 1.1)\n",
    "ax2.set_xlabel('x')\n",
    "ax2.set_ylabel('y')\n",
    "ax2.set_title('2D Collocation Points')\n",
    "ax2.grid(True)\n",
    "ax2.set_aspect('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Loss Weighting\n",
    "\n",
    "Different components of the loss function may need different weights for optimal training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the effect of loss weighting\n",
    "def weighted_loss_example():\n",
    "    # Example loss components\n",
    "    epochs = np.arange(100)\n",
    "    \n",
    "    # Different weighting schemes\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    weights = [\n",
    "        {'pde': 1.0, 'bc': 1.0, 'ic': 1.0},\n",
    "        {'pde': 1.0, 'bc': 10.0, 'ic': 10.0},\n",
    "        {'pde': 0.1, 'bc': 50.0, 'ic': 50.0}\n",
    "    ]\n",
    "    \n",
    "    for idx, w in enumerate(weights):\n",
    "        # Simulate loss components\n",
    "        pde_loss = 0.1 * np.exp(-epochs/20) + 0.001\n",
    "        bc_loss = 0.5 * np.exp(-epochs/15) + 0.0001\n",
    "        ic_loss = 0.3 * np.exp(-epochs/10) + 0.0001\n",
    "        \n",
    "        total_loss = w['pde']*pde_loss + w['bc']*bc_loss + w['ic']*ic_loss\n",
    "        \n",
    "        ax = axes[idx]\n",
    "        ax.semilogy(epochs, total_loss, 'k-', linewidth=2, label='Total')\n",
    "        ax.semilogy(epochs, pde_loss, 'r--', label='PDE')\n",
    "        ax.semilogy(epochs, bc_loss, 'g--', label='BC')\n",
    "        ax.semilogy(epochs, ic_loss, 'b--', label='IC')\n",
    "        \n",
    "        ax.set_xlabel('Epoch')\n",
    "        ax.set_ylabel('Loss')\n",
    "        ax.set_title(f\"Weights: PDE={w['pde']}, BC={w['bc']}, IC={w['ic']}\")\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "weighted_loss_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we've covered:\n",
    "\n",
    "1. **What PINNs are**: Neural networks that learn to satisfy PDEs\n",
    "2. **How they work**: Using physics-informed loss functions and automatic differentiation\n",
    "3. **Their advantages**: Continuous solutions, mesh-free, handling complex geometries\n",
    "4. **A simple example**: Solving an ODE with PINNs\n",
    "5. **Key concepts**: Automatic differentiation, collocation points, and loss weighting\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In the next notebook, we'll apply these concepts to solve the wave equation, a more complex partial differential equation that describes wave propagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. **Modify the ODE example** to solve: $\\frac{du}{dx} - 2u = -4, u(0) = 1$\n",
    "   - Hint: The analytical solution is $u(x) = 2 - e^{2x}$\n",
    "\n",
    "2. **Experiment with network architecture**: Try different numbers of layers and neurons\n",
    "\n",
    "3. **Investigate collocation points**: How does the number of collocation points affect accuracy?\n",
    "\n",
    "4. **Loss weighting**: For the ODE example, what happens if you weight the boundary condition loss differently?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Space for exercises\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
