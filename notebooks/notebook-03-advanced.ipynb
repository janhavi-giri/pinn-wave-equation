{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced PINN Techniques\n",
    "\n",
    "This notebook explores advanced techniques for improving PINN performance and handling more complex scenarios.\n",
    "\n",
    "## Topics Covered\n",
    "1. Adaptive loss weighting\n",
    "2. Advanced network architectures\n",
    "3. Domain decomposition\n",
    "4. Transfer learning for PDEs\n",
    "5. Inverse problems\n",
    "6. Multi-physics coupling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import our modules\n",
    "from src.model import WavePINN\n",
    "from src.losses import PhysicsInformedLoss\n",
    "from src.train import train_model\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "plt.rcParams['figure.dpi'] = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Adaptive Loss Weighting\n",
    "\n",
    "One of the biggest challenges in PINNs is balancing different loss components. Adaptive weighting adjusts weights during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveWeightedLoss:\n",
    "    \"\"\"Adaptive loss weighting based on gradient magnitudes\"\"\"\n",
    "    \n",
    "    def __init__(self, initial_weights=None):\n",
    "        if initial_weights is None:\n",
    "            initial_weights = {'pde': 1.0, 'ic': 1.0, 'bc': 1.0}\n",
    "        \n",
    "        self.weights = initial_weights.copy()\n",
    "        self.grad_history = {key: [] for key in self.weights}\n",
    "        self.update_freq = 100\n",
    "        self.iteration = 0\n",
    "    \n",
    "    def compute_gradients(self, losses, model):\n",
    "        \"\"\"Compute gradient magnitudes for each loss component\"\"\"\n",
    "        grads = {}\n",
    "        \n",
    "        for key, loss in losses.items():\n",
    "            if key != 'total':\n",
    "                # Compute gradients\n",
    "                grad = torch.autograd.grad(loss, model.parameters(), \n",
    "                                         retain_graph=True, allow_unused=True)\n",
    "                \n",
    "                # Compute gradient magnitude\n",
    "                grad_norm = 0\n",
    "                for g in grad:\n",
    "                    if g is not None:\n",
    "                        grad_norm += g.norm().item()**2\n",
    "                \n",
    "                grads[key] = np.sqrt(grad_norm)\n",
    "        \n",
    "        return grads\n",
    "    \n",
    "    def update_weights(self, losses, model):\n",
    "        \"\"\"Update weights based on gradient balancing\"\"\"\n",
    "        self.iteration += 1\n",
    "        \n",
    "        # Compute gradients\n",
    "        grads = self.compute_gradients(losses, model)\n",
    "        \n",
    "        # Store history\n",
    "        for key in grads:\n",
    "            self.grad_history[key].append(grads[key])\n",
    "        \n",
    "        # Update weights periodically\n",
    "        if self.iteration % self.update_freq == 0 and self.iteration > 0:\n",
    "            # Compute mean gradients over recent history\n",
    "            mean_grads = {}\n",
    "            for key in self.weights:\n",
    "                if key in self.grad_history and len(self.grad_history[key]) > 0:\n",
    "                    mean_grads[key] = np.mean(self.grad_history[key][-50:])\n",
    "            \n",
    "            # Balance gradients\n",
    "            if len(mean_grads) > 0:\n",
    "                max_grad = max(mean_grads.values())\n",
    "                for key in self.weights:\n",
    "                    if key in mean_grads and mean_grads[key] > 0:\n",
    "                        self.weights[key] *= max_grad / mean_grads[key]\n",
    "                \n",
    "                # Normalize weights\n",
    "                total_weight = sum(self.weights.values())\n",
    "                for key in self.weights:\n",
    "                    self.weights[key] /= total_weight\n",
    "        \n",
    "        return self.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate adaptive weighting\n",
    "def train_with_adaptive_weights(model, epochs=2000):\n",
    "    \"\"\"Train PINN with adaptive loss weighting\"\"\"\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    adaptive_loss = AdaptiveWeightedLoss()\n",
    "    \n",
    "    history = {'loss': [], 'weights_pde': [], 'weights_ic': [], 'weights_bc': []}\n",
    "    \n",
    "    for epoch in tqdm(range(epochs), desc='Training with adaptive weights'):\n",
    "        # Generate training data\n",
    "        x_pde = torch.rand(1000, 1, device=device, requires_grad=True)\n",
    "        t_pde = torch.rand(1000, 1, device=device, requires_grad=True)\n",
    "        x_ic = torch.rand(200, 1, device=device)\n",
    "        x_bc = torch.cat([torch.zeros(100, 1), torch.ones(100, 1)], dim=0).to(device)\n",
    "        t_bc = torch.rand(200, 1, device=device)\n",
    "        \n",
    "        # Compute individual losses\n",
    "        losses = {}\n",
    "        \n",
    "        # PDE loss\n",
    "        u = model(x_pde, t_pde)\n",
    "        u_x = torch.autograd.grad(u, x_pde, torch.ones_like(u), create_graph=True)[0]\n",
    "        u_t = torch.autograd.grad(u, t_pde, torch.ones_like(u), create_graph=True)[0]\n",
    "        u_xx = torch.autograd.grad(u_x, x_pde, torch.ones_like(u_x), create_graph=True)[0]\n",
    "        u_tt = torch.autograd.grad(u_t, t_pde, torch.ones_like(u_t), create_graph=True)[0]\n",
    "        losses['pde'] = torch.mean((u_tt - u_xx)**2)\n",
    "        \n",
    "        # IC loss\n",
    "        t_ic = torch.zeros_like(x_ic)\n",
    "        u_ic = model(x_ic, t_ic)\n",
    "        losses['ic'] = torch.mean((u_ic - torch.sin(np.pi * x_ic))**2)\n",
    "        \n",
    "        # BC loss\n",
    "        u_bc = model(x_bc, t_bc)\n",
    "        losses['bc'] = torch.mean(u_bc**2)\n",
    "        \n",
    "        # Update weights\n",
    "        weights = adaptive_loss.update_weights(losses, model)\n",
    "        \n",
    "        # Total loss\n",
    "        total_loss = sum(weights[key] * losses[key] for key in losses)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Record history\n",
    "        history['loss'].append(total_loss.item())\n",
    "        history['weights_pde'].append(weights['pde'])\n",
    "        history['weights_ic'].append(weights['ic'])\n",
    "        history['weights_bc'].append(weights['bc'])\n",
    "    \n",
    "    return history\n",
    "\n",
    "# Train model with adaptive weights\n",
    "model_adaptive = WavePINN().to(device)\n",
    "history_adaptive = train_with_adaptive_weights(model_adaptive)\n",
    "\n",
    "# Visualize weight evolution\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Loss history\n",
    "ax1.semilogy(history_adaptive['loss'])\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Total Loss')\n",
    "ax1.set_title('Training Loss with Adaptive Weighting')\n",
    "ax1.grid(True)\n",
    "\n",
    "# Weight evolution\n",
    "ax2.plot(history_adaptive['weights_pde'], label='PDE weight')\n",
    "ax2.plot(history_adaptive['weights_ic'], label='IC weight')\n",
    "ax2.plot(history_adaptive['weights_bc'], label='BC weight')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Weight Value')\n",
    "ax2.set_title('Adaptive Weight Evolution')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Advanced Network Architectures\n",
    "\n",
    "Different architectures can significantly impact PINN performance. Let's explore some advanced options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualPINN(nn.Module):\n",
    "    \"\"\"PINN with residual connections\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim=2, hidden_dim=64, num_blocks=3):\n",
    "        super(ResidualPINN, self).__init__()\n",
    "        \n",
    "        self.input_layer = nn.Linear(input_dim, hidden_dim)\n",
    "        \n",
    "        # Residual blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(hidden_dim, hidden_dim)\n",
    "            ) for _ in range(num_blocks)\n",
    "        ])\n",
    "        \n",
    "        self.output_layer = nn.Linear(hidden_dim, 1)\n",
    "        self.activation = nn.Tanh()\n",
    "    \n",
    "    def forward(self, x, t):\n",
    "        # Input\n",
    "        inputs = torch.cat([x, t], dim=1)\n",
    "        out = self.activation(self.input_layer(inputs))\n",
    "        \n",
    "        # Residual blocks\n",
    "        for block in self.blocks:\n",
    "            residual = out\n",
    "            out = block(out)\n",
    "            out = self.activation(out + residual)  # Residual connection\n",
    "        \n",
    "        # Output\n",
    "        out = self.output_layer(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class FourierFeaturePINN(nn.Module):\n",
    "    \"\"\"PINN with Fourier feature encoding\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim=2, hidden_dim=64, num_frequencies=10):\n",
    "        super(FourierFeaturePINN, self).__init__()\n",
    "        \n",
    "        # Fourier feature frequencies\n",
    "        self.frequencies = 2**torch.linspace(0, num_frequencies-1, num_frequencies)\n",
    "        self.frequencies = self.frequencies.reshape(1, -1)\n",
    "        \n",
    "        # Network\n",
    "        feature_dim = input_dim * num_frequencies * 2\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(feature_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "    \n",
    "    def fourier_features(self, x):\n",
    "        \"\"\"Compute Fourier features\"\"\"\n",
    "        self.frequencies = self.frequencies.to(x.device)\n",
    "        x_proj = x @ self.frequencies\n",
    "        return torch.cat([torch.sin(2 * np.pi * x_proj), \n",
    "                         torch.cos(2 * np.pi * x_proj)], dim=-1)\n",
    "    \n",
    "    def forward(self, x, t):\n",
    "        # Compute Fourier features\n",
    "        inputs = torch.cat([x, t], dim=1)\n",
    "        features = self.fourier_features(inputs)\n",
    "        features = features.reshape(features.shape[0], -1)\n",
    "        \n",
    "        return self.net(features)\n",
    "\n",
    "\n",
    "# Compare architectures\n",
    "architectures = {\n",
    "    'Standard': WavePINN(),\n",
    "    'Residual': ResidualPINN(),\n",
    "    'Fourier': FourierFeaturePINN()\n",
    "}\n",
    "\n",
    "# Count parameters\n",
    "for name, model in architectures.items():\n",
    "    params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"{name} PINN: {params:,} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Domain Decomposition\n",
    "\n",
    "For complex domains or large-scale problems, domain decomposition can improve training efficiency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DomainDecompositionPINN:\n",
    "    \"\"\"PINN with domain decomposition\"\"\"\n",
    "    \n",
    "    def __init__(self, num_subdomains=4):\n",
    "        self.num_subdomains = num_subdomains\n",
    "        self.models = []\n",
    "        self.boundaries = []\n",
    "        \n",
    "        # Create subdomains\n",
    "        for i in range(num_subdomains):\n",
    "            self.models.append(WavePINN().to(device))\n",
    "            \n",
    "            # Define subdomain boundaries\n",
    "            x_min = i / num_subdomains\n",
    "            x_max = (i + 1) / num_subdomains\n",
    "            self.boundaries.append((x_min, x_max))\n",
    "    \n",
    "    def forward(self, x, t):\n",
    "        \"\"\"Evaluate the appropriate subdomain model\"\"\"\n",
    "        outputs = torch.zeros_like(x)\n",
    "        \n",
    "        for i, (model, (x_min, x_max)) in enumerate(zip(self.models, self.boundaries)):\n",
    "            # Find points in this subdomain\n",
    "            mask = (x >= x_min) & (x <= x_max)\n",
    "            \n",
    "            if mask.any():\n",
    "                x_sub = x[mask]\n",
    "                t_sub = t[mask]\n",
    "                \n",
    "                # Transform to local coordinates\n",
    "                x_local = (x_sub - x_min) / (x_max - x_min)\n",
    "                \n",
    "                # Evaluate model\n",
    "                outputs[mask] = model(x_local, t_sub)\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    def train_step(self, optimizer, loss_fn):\n",
    "        \"\"\"Train all subdomain models with interface conditions\"\"\"\n",
    "        total_loss = 0\n",
    "        \n",
    "        for i, (model, (x_min, x_max)) in enumerate(zip(self.models, self.boundaries)):\n",
    "            # Generate training data for subdomain\n",
    "            x_pde = torch.rand(250, 1, device=device) * (x_max - x_min) + x_min\n",
    "            t_pde = torch.rand(250, 1, device=device)\n",
    "            \n",
    "            # Subdomain loss\n",
    "            loss = loss_fn(model, x_pde, t_pde)\n",
    "            \n",
    "            # Interface conditions (continuity)\n",
    "            if i < self.num_subdomains - 1:\n",
    "                x_interface = torch.full((50, 1), x_max, device=device)\n",
    "                t_interface = torch.rand(50, 1, device=device)\n",
    "                \n",
    "                # Values from current and next subdomain\n",
    "                u_current = model(torch.ones_like(x_interface), t_interface)\n",
    "                u_next = self.models[i+1](torch.zeros_like(x_interface), t_interface)\n",
    "                \n",
    "                # Continuity loss\n",
    "                interface_loss = torch.mean((u_current - u_next)**2)\n",
    "                loss += 10.0 * interface_loss\n",
    "            \n",
    "            total_loss += loss\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        return total_loss.item()\n",
    "\n",
    "# Visualize domain decomposition\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Create domain decomposition\n",
    "dd_pinn = DomainDecompositionPINN(num_subdomains=4)\n",
    "\n",
    "# Plot subdomains\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, dd_pinn.num_subdomains))\n",
    "for i, ((x_min, x_max), color) in enumerate(zip(dd_pinn.boundaries, colors)):\n",
    "    ax.add_patch(plt.Rectangle((x_min, 0), x_max-x_min, 1, \n",
    "                              facecolor=color, alpha=0.3, edgecolor='black', linewidth=2))\n",
    "    ax.text((x_min + x_max)/2, 0.5, f'Domain {i+1}', \n",
    "            ha='center', va='center', fontsize=12, weight='bold')\n",
    "\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_xlabel('x', fontsize=12)\n",
    "ax.set_ylabel('t', fontsize=12)\n",
    "ax.set_title('Domain Decomposition for PINN', fontsize=14)\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "# Add interface lines\n",
    "for i in range(1, dd_pinn.num_subdomains):\n",
    "    x_interface = i / dd_pinn.num_subdomains\n",
    "    ax.axvline(x=x_interface, color='red', linestyle='--', linewidth=2, alpha=0.7)\n",
    "    ax.text(x_interface, 1.05, 'Interface', ha='center', color='red', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Transfer Learning for PDEs\n",
    "\n",
    "We can use a pre-trained PINN as a starting point for solving related problems:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_learning_demo():\n",
    "    \"\"\"Demonstrate transfer learning for PDEs\"\"\"\n",
    "    \n",
    "    # Step 1: Train base model on standard wave equation (c=1)\n",
    "    print(\"Step 1: Training base model (c=1)...\")\n",
    "    base_model = WavePINN().to(device)\n",
    "    history_base = train_model(base_model, epochs=2000, verbose=False)\n",
    "    \n",
    "    # Step 2: Use as initialization for different wave speed (c=2)\n",
    "    print(\"\\nStep 2: Transfer learning for c=2...\")\n",
    "    \n",
    "    # Create two models: one with transfer, one without\n",
    "    model_transfer = WavePINN().to(device)\n",
    "    model_transfer.load_state_dict(base_model.state_dict())  # Transfer weights\n",
    "    \n",
    "    model_scratch = WavePINN().to(device)  # Train from scratch\n",
    "    \n",
    "    # Modified loss for c=2\n",
    "    from src.losses import PhysicsInformedLoss\n",
    "    loss_fn_c2 = PhysicsInformedLoss(wave_speed=2.0)\n",
    "    \n",
    "    # Train both models\n",
    "    history_transfer = []\n",
    "    history_scratch = []\n",
    "    \n",
    "    optimizer_transfer = torch.optim.Adam(model_transfer.parameters(), lr=1e-3)\n",
    "    optimizer_scratch = torch.optim.Adam(model_scratch.parameters(), lr=1e-3)\n",
    "    \n",
    "    for epoch in tqdm(range(1000), desc='Training'):\n",
    "        # Generate data\n",
    "        x_pde = torch.rand(1000, 1, device=device)\n",
    "        t_pde = torch.rand(1000, 1, device=device)\n",
    "        x_ic = torch.rand(200, 1, device=device)\n",
    "        x_bc = torch.cat([torch.zeros(100, 1), torch.ones(100, 1)], dim=0).to(device)\n",
    "        t_bc = torch.rand(200, 1, device=device)\n",
    "        \n",
    "        # Train transfer model\n",
    "        loss_transfer, _ = loss_fn_c2(model_transfer, x_pde, t_pde, x_ic, x_bc, t_bc)\n",
    "        optimizer_transfer.zero_grad()\n",
    "        loss_transfer.backward()\n",
    "        optimizer_transfer.step()\n",
    "        history_transfer.append(loss_transfer.item())\n",
    "        \n",
    "        # Train scratch model\n",
    "        loss_scratch, _ = loss_fn_c2(model_scratch, x_pde, t_pde, x_ic, x_bc, t_bc)\n",
    "        optimizer_scratch.zero_grad()\n",
    "        loss_scratch.backward()\n",
    "        optimizer_scratch.step()\n",
    "        history_scratch.append(loss_scratch.item())\n",
    "    \n",
    "    return history_transfer, history_scratch\n",
    "\n",
    "# Run transfer learning experiment\n",
    "history_transfer, history_scratch = transfer_learning_demo()\n",
    "\n",
    "# Visualize results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Training curves\n",
    "ax1.semilogy(history_transfer, 'b-', linewidth=2, label='With Transfer Learning')\n",
    "ax1.semilogy(history_scratch, 'r--', linewidth=2, label='From Scratch')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Transfer Learning Comparison')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Convergence speed\n",
    "threshold = 0.01\n",
    "epochs_transfer = next((i for i, loss in enumerate(history_transfer) if loss < threshold), len(history_transfer))\n",
    "epochs_scratch = next((i for i, loss in enumerate(history_scratch) if loss < threshold), len(history_scratch))\n",
    "\n",
    "ax2.bar(['Transfer Learning', 'From Scratch'], [epochs_transfer, epochs_scratch], \n",
    "        color=['blue', 'red'], alpha=0.7)\n",
    "ax2.set_ylabel('Epochs to Reach Loss < 0.01')\n",
    "ax2.set_title('Convergence Speed')\n",
    "\n",
    "# Add text\n",
    "speedup = epochs_scratch / epochs_transfer if epochs_transfer > 0 else float('inf')\n",
    "ax2.text(0.5, max(epochs_transfer, epochs_scratch) * 0.5, \n",
    "         f'{speedup:.1f}x speedup', ha='center', fontsize=14, weight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Inverse Problems\n",
    "\n",
    "PINNs can solve inverse problems - inferring unknown parameters from observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InversePINN(nn.Module):\n",
    "    \"\"\"PINN for inverse problems - learns both solution and parameters\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(InversePINN, self).__init__()\n",
    "        \n",
    "        # Solution network\n",
    "        self.solution_net = WavePINN()\n",
    "        \n",
    "        # Unknown parameter (wave speed)\n",
    "        self.log_c = nn.Parameter(torch.tensor([0.0]))  # log(c) for positivity\n",
    "    \n",
    "    def forward(self, x, t):\n",
    "        return self.solution_net(x, t)\n",
    "    \n",
    "    @property\n",
    "    def wave_speed(self):\n",
    "        return torch.exp(self.log_c)\n",
    "\n",
    "def solve_inverse_problem():\n",
    "    \"\"\"Solve inverse problem: find wave speed from observations\"\"\"\n",
    "    \n",
    "    # True wave speed (unknown to the model)\n",
    "    true_c = 1.5\n",
    "    \n",
    "    # Generate synthetic observations\n",
    "    n_obs = 100\n",
    "    x_obs = torch.rand(n_obs, 1) * 0.8 + 0.1  # Avoid boundaries\n",
    "    t_obs = torch.rand(n_obs, 1) * 0.8 + 0.1\n",
    "    \n",
    "    # True solution\n",
    "    u_obs = torch.sin(np.pi * x_obs) * torch.cos(np.pi * true_c * t_obs)\n",
    "    u_obs += 0.01 * torch.randn_like(u_obs)  # Add noise\n",
    "    \n",
    "    # Create inverse PINN\n",
    "    model = InversePINN().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    \n",
    "    # Training\n",
    "    history_c = []\n",
    "    history_loss = []\n",
    "    \n",
    "    for epoch in tqdm(range(3000), desc='Solving inverse problem'):\n",
    "        # PDE collocation points\n",
    "        x_pde = torch.rand(500, 1, device=device, requires_grad=True)\n",
    "        t_pde = torch.rand(500, 1, device=device, requires_grad=True)\n",
    "        \n",
    "        # Forward pass\n",
    "        u = model(x_pde, t_pde)\n",
    "        \n",
    "        # Compute derivatives\n",
    "        u_x = torch.autograd.grad(u, x_pde, torch.ones_like(u), create_graph=True)[0]\n",
    "        u_t = torch.autograd.grad(u, t_pde, torch.ones_like(u), create_graph=True)[0]\n",
    "        u_xx = torch.autograd.grad(u_x, x_pde, torch.ones_like(u_x), create_graph=True)[0]\n",
    "        u_tt = torch.autograd.grad(u_t, t_pde, torch.ones_like(u_t), create_graph=True)[0]\n",
    "        \n",
    "        # PDE residual with unknown parameter\n",
    "        c_squared = model.wave_speed**2\n",
    "        pde_loss = torch.mean((u_tt - c_squared * u_xx)**2)\n",
    "        \n",
    "        # Data loss\n",
    "        x_obs_gpu = x_obs.to(device)\n",
    "        t_obs_gpu = t_obs.to(device)\n",
    "        u_obs_gpu = u_obs.to(device)\n",
    "        \n",
    "        u_pred = model(x_obs_gpu, t_obs_gpu)\n",
    "        data_loss = torch.mean((u_pred - u_obs_gpu)**2)\n",
    "        \n",
    "        # Initial condition\n",
    "        x_ic = torch.rand(100, 1, device=device)\n",
    "        t_ic = torch.zeros_like(x_ic)\n",
    "        u_ic = model(x_ic, t_ic)\n",
    "        ic_loss = torch.mean((u_ic - torch.sin(np.pi * x_ic))**2)\n",
    "        \n",
    "        # Total loss\n",
    "        loss = pde_loss + 100 * data_loss + 10 * ic_loss\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Record history\n",
    "        history_c.append(model.wave_speed.item())\n",
    "        history_loss.append(loss.item())\n",
    "    \n",
    "    return model, history_c, history_loss, true_c\n",
    "\n",
    "# Solve inverse problem\n",
    "model_inverse, history_c, history_loss, true_c = solve_inverse_problem()\n",
    "\n",
    "# Visualize results\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Loss history\n",
    "ax1.semilogy(history_loss)\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Total Loss')\n",
    "ax1.set_title('Training Loss')\n",
    "ax1.grid(True)\n",
    "\n",
    "# Parameter estimation\n",
    "ax2.plot(history_c, 'b-', linewidth=2)\n",
    "ax2.axhline(y=true_c, color='r', linestyle='--', linewidth=2, label=f'True c = {true_c}')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Estimated Wave Speed c')\n",
    "ax2.set_title('Parameter Estimation')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "# Final estimate\n",
    "final_c = history_c[-1]\n",
    "error = abs(final_c - true_c) / true_c * 100\n",
    "\n",
    "ax3.bar(['True', 'Estimated'], [true_c, final_c], color=['red', 'blue'], alpha=0.7)\n",
    "ax3.set_ylabel('Wave Speed c')\n",
    "ax3.set_title(f'Final Result (Error: {error:.2f}%)')\n",
    "ax3.set_ylim(0, 2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTrue wave speed: {true_c}\")\n",
    "print(f\"Estimated wave speed: {final_c:.4f}\")\n",
    "print(f\"Relative error: {error:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Multi-Physics Coupling\n",
    "\n",
    "PINNs can handle coupled PDEs, making them suitable for multi-physics problems:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoupledPINN(nn.Module):\n",
    "    \"\"\"PINN for coupled wave-heat equations\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(CoupledPINN, self).__init__()\n",
    "        \n",
    "        # Shared encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(2, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        # Separate decoders for each field\n",
    "        self.wave_decoder = nn.Sequential(\n",
    "            nn.Linear(64, 32),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "        \n",
    "        self.heat_decoder = nn.Sequential(\n",
    "            nn.Linear(64, 32),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, t):\n",
    "        inputs = torch.cat([x, t], dim=1)\n",
    "        features = self.encoder(inputs)\n",
    "        \n",
    "        u_wave = self.wave_decoder(features)\n",
    "        u_heat = self.heat_decoder(features)\n",
    "        \n",
    "        return u_wave, u_heat\n",
    "\n",
    "# Visualize multi-physics coupling\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Draw coupling diagram\n",
    "from matplotlib.patches import FancyBboxPatch, FancyArrowPatch\n",
    "\n",
    "# Wave equation box\n",
    "wave_box = FancyBboxPatch((0.1, 0.6), 0.3, 0.3, \n",
    "                          boxstyle=\"round,pad=0.05\",\n",
    "                          facecolor='lightblue', edgecolor='black', linewidth=2)\n",
    "ax.add_patch(wave_box)\n",
    "ax.text(0.25, 0.75, 'Wave Equation\\n∂²u/∂t² = c²∇²u', \n",
    "        ha='center', va='center', fontsize=12, weight='bold')\n",
    "\n",
    "# Heat equation box\n",
    "heat_box = FancyBboxPatch((0.6, 0.6), 0.3, 0.3,\n",
    "                          boxstyle=\"round,pad=0.05\",\n",
    "                          facecolor='lightcoral', edgecolor='black', linewidth=2)\n",
    "ax.add_patch(heat_box)\n",
    "ax.text(0.75, 0.75, 'Heat Equation\\n∂T/∂t = α∇²T', \n",
    "        ha='center', va='center', fontsize=12, weight='bold')\n",
    "\n",
    "# Coupling arrows\n",
    "arrow1 = FancyArrowPatch((0.4, 0.75), (0.6, 0.75),\n",
    "                         connectionstyle=\"arc3,rad=.2\",\n",
    "                         arrowstyle=\"->\", mutation_scale=20,\n",
    "                         linewidth=2, color='green')\n",
    "ax.add_patch(arrow1)\n",
    "\n",
    "arrow2 = FancyArrowPatch((0.6, 0.75), (0.4, 0.75),\n",
    "                         connectionstyle=\"arc3,rad=-.2\",\n",
    "                         arrowstyle=\"->\", mutation_scale=20,\n",
    "                         linewidth=2, color='green')\n",
    "ax.add_patch(arrow2)\n",
    "\n",
    "# Coupling term\n",
    "ax.text(0.5, 0.85, 'Coupling: u affects α', ha='center', fontsize=10, color='green')\n",
    "ax.text(0.5, 0.65, 'Coupling: T affects c', ha='center', fontsize=10, color='green')\n",
    "\n",
    "# PINN box\n",
    "pinn_box = FancyBboxPatch((0.2, 0.1), 0.6, 0.3,\n",
    "                          boxstyle=\"round,pad=0.05\",\n",
    "                          facecolor='lightyellow', edgecolor='black', linewidth=2)\n",
    "ax.add_patch(pinn_box)\n",
    "ax.text(0.5, 0.25, 'Coupled PINN\\n(Shared representation)', \n",
    "        ha='center', va='center', fontsize=14, weight='bold')\n",
    "\n",
    "# Connections to PINN\n",
    "ax.arrow(0.25, 0.6, 0, -0.15, head_width=0.03, head_length=0.02, fc='black', ec='black')\n",
    "ax.arrow(0.75, 0.6, 0, -0.15, head_width=0.03, head_length=0.02, fc='black', ec='black')\n",
    "\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.axis('off')\n",
    "ax.set_title('Multi-Physics Coupling with PINNs', fontsize=16, weight='bold', pad=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we explored advanced PINN techniques:\n",
    "\n",
    "1. **Adaptive Loss Weighting**: Automatically balances different loss components during training\n",
    "2. **Advanced Architectures**: Residual connections and Fourier features can improve performance\n",
    "3. **Domain Decomposition**: Enables solving large-scale problems by dividing the domain\n",
    "4. **Transfer Learning**: Pre-trained models can accelerate training for related problems\n",
    "5. **Inverse Problems**: PINNs can infer unknown parameters from observations\n",
    "6. **Multi-Physics**: Coupled PDEs can be solved with shared representations\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **Flexibility**: PINNs can be adapted to various problem types and scales\n",
    "- **Efficiency**: Advanced techniques can significantly improve training speed and accuracy\n",
    "- **Versatility**: From forward to inverse problems, PINNs handle diverse scenarios\n",
    "- **Extensibility**: The framework naturally extends to coupled and multi-physics problems\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Explore these techniques on your own PDE problems\n",
    "- Combine multiple techniques for even better performance\n",
    "- Consider hardware acceleration (multi-GPU training)\n",
    "- Investigate recent research papers for cutting-edge methods"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
